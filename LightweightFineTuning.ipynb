{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA is selected for its efficiency in fine-tuning transformer models by adding trainable low-rank matrices to the attention layers while keeping the base model frozen.\n",
    "* Model: GPT-2 is selected due to its strong contextual understanding and pretrained knowledge, making it adaptable for text classification tasks like spam detection.\n",
    "* Evaluation approach: The Transformer Trainer is used as it provides an optimized training loop for fine-tuning transformer models with built-in support for distributed training, mixed precision, and evaluation.\n",
    "* Fine-tuning dataset: ucirvine/sms_spam is well labled dataset with a balanced property to distinguish between spam and not spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "847bed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==1.2.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.4.4 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.4.4)\n",
      "Requirement already satisfied: aiohttp==3.11.11 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.11.11)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: async-timeout==5.0.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: attrs==24.3.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (24.3.0)\n",
      "Requirement already satisfied: certifi==2024.12.14 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: datasets==2.15.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (2.15.0)\n",
      "Requirement already satisfied: debugpy==1.8.11 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (1.8.11)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.7 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.3.7)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.1.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (2.1.0)\n",
      "Requirement already satisfied: filelock==3.16.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (3.16.1)\n",
      "Requirement already satisfied: fonttools==4.55.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.55.3)\n",
      "Requirement already satisfied: frozenlist==1.5.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (1.5.0)\n",
      "Requirement already satisfied: fsspec==2023.10.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub==0.27.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (0.27.1)\n",
      "Requirement already satisfied: idna==3.10 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (3.10)\n",
      "Requirement already satisfied: importlib_metadata==8.5.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (8.5.0)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.18.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (8.18.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.5 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (3.1.5)\n",
      "Requirement already satisfied: joblib==1.4.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.4.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (1.4.7)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.6.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (3.6.3)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (1.3.0)\n",
      "Requirement already satisfied: multidict==6.1.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (6.1.0)\n",
      "Requirement already satisfied: multiprocess==0.70.15 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (0.70.15)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.2.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (3.2.1)\n",
      "Requirement already satisfied: numpy==1.24.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (1.24.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (12.4.127)\n",
      "Requirement already satisfied: packaging==24.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (24.2)\n",
      "Requirement already satisfied: pandas==1.5.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (1.5.2)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (0.8.4)\n",
      "Requirement already satisfied: peft==0.14.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (0.14.0)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 59)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.1.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (11.1.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 61)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 62)) (3.0.48)\n",
      "Requirement already satisfied: propcache==0.2.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (0.2.1)\n",
      "Requirement already satisfied: psutil==6.1.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 64)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 65)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 66)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow==18.1.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 67)) (18.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix==0.6 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 68)) (0.6)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 69)) (2.19.1)\n",
      "Requirement already satisfied: pyparsing==3.2.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2024.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 72)) (2024.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 73)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 74)) (26.2.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 75)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 76)) (2.32.3)\n",
      "Requirement already satisfied: safetensors==0.5.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 77)) (0.5.2)\n",
      "Requirement already satisfied: scikit-learn==1.2.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 78)) (1.2.0)\n",
      "Requirement already satisfied: scipy==1.13.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (1.13.1)\n",
      "Requirement already satisfied: six==1.17.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 80)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 81)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 82)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl==3.5.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 83)) (3.5.0)\n",
      "Requirement already satisfied: tokenizers==0.21.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 84)) (0.21.0)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 85)) (2.5.1)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 86)) (0.20.1)\n",
      "Requirement already satisfied: tornado==6.4.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 87)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 88)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 89)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.47.1 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 90)) (4.47.1)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 91)) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 92)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 93)) (2.3.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 94)) (0.2.13)\n",
      "Requirement already satisfied: xxhash==3.5.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 95)) (3.5.0)\n",
      "Requirement already satisfied: yarl==1.18.3 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 96)) (1.18.3)\n",
      "Requirement already satisfied: zipp==3.21.0 in /home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 97)) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f65a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Debugging: Makes CUDA ops synchronous for better error tracking\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "# Enables CUDA Dynamic Shared Allocation for memory debugging\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7346a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the pre-defined model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# to ensure that the padding token used by a tokenizer is set to the same value as the end-of-sequence token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sms\"], padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2bc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train and test sets\n",
    "train_dataset = dataset[\"train\"].map(tokenize, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d7b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-defined model with custom output labels\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")# To ensures that the model's configuration recognizes the padding token that was set in the tokenizer.\n",
    "foundation_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(foundation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e57506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# get predictions from the foundation model\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "for example in test_dataset:    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    foundation_model.to(device)\n",
    "\n",
    "    # Tokenize the input data\n",
    "    inputs = tokenizer(example[\"sms\"], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get raw predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = foundation_model(**inputs)\n",
    "        logits = outputs.logits        \n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)    \n",
    "    predicted_class_id = probabilities.argmax().item()\n",
    "    \n",
    "    predictions.append(predicted_class_id)\n",
    "    labels.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ede5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8834080717488789}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Compute evaluation metrics\n",
    "evaluation_metrics = compute_metrics(labels, predictions)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,992 || all params: 124,590,336 || trainable%: 0.1196\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# PEFT model configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Load again the pre-defined foundation model with custom output labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "peft_model = PeftModelForSequenceClassification(model, peft_config)\n",
    "\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7425ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clears unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "# Resets peak memory tracking for profiling\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsefati/workspace/gen_ai/.venv/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_143244/2234520589.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  peft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 04:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.978100</td>\n",
       "      <td>0.762144</td>\n",
       "      <td>0.869058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=0.950489262172154, metrics={'train_runtime': 287.0679, 'train_samples_per_second': 15.533, 'train_steps_per_second': 0.488, 'total_flos': 588140786128896.0, 'train_loss': 0.950489262172154, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/peft_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs/peft_model',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with compute_metrics\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7621443271636963, 'eval_accuracy': 0.8690582959641255, 'eval_runtime': 18.2539, 'eval_samples_per_second': 61.083, 'eval_steps_per_second': 1.917, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = peft_trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3989711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model locally\n",
    "peft_model.save_pretrained('model/peft_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c0eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load the model from the local machine\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"model/peft_model\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict(prompt: str) -> str:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    inference_model.to(device)\n",
    "\n",
    "    # Prepare the input text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**inputs)\n",
    "        logits = outputs.logits        \n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)    \n",
    "    predicted_class_id = probabilities.argmax().item()    \n",
    "    id2label={0: \"spam\", 1: \"not spam\"}\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Yup next stop.'\n",
      "Predicted label: spam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Non spam inference example\n",
    "prompt = \"Yup next stop.\"\n",
    "predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV'\n",
      "Predicted label: spam\n"
     ]
    }
   ],
   "source": [
    "# Spam inference example\n",
    "prompt = \"SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV\"\n",
    "predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
